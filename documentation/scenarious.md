# Сценарии использования

## XRay-Core

XRay-Core - это многопользовательский прокси-сервер (каждый пользователь характеризуется сочетанием ID + Key). Список пользователей можно задать при старте ядра и впоследствии редактировать (добавлять и удалять пользователей) через gRPC-API. Запускается в моём случае как systemd-сервис. Может иногда падать по разным причинам, и тогда его требуется перезапускать. **Stateful, Not Persistent**. Это сторонний компонент, его планируется использовать as-is.

## XRay-Node

Сервер (физический), на котором работает XRay-Core. Обычно слабая VPS, 1 vCPU, 1-2 GB RAM. На нём запущен агент (тоже systemd-сервис, stateless, с автозапуском), который предоставлет возможность управления ядром с помощью методов:

```cpp
// запустить сервис с указанным списком пользователей
POST /start { users }

// остановить сервис
POST /stop

// запросить статус сервиса (Running/Stopped)
GET /status

// изменить список пользователей
POST /users/edit {add_users, del_users}
```

## XRay-Manager

Сервис, который администрирует ноды - взаимодействует с ними и с базой данных. 
Позволяет:
- добавлять/удалять/запускать/останавливать ноды
- создавать/удалять пользователей
- мониторить состояние нод, перезапускать их, если возможно
- обрабатывать запросы пользователей - возвращать список работающих нод, на которых данный пользователь в данный момент добавлен

С нодами менеджер взаимодействует односторонне, вызывая их методы (авторизация и взаимодействие менеджера с нодами - с помощью JWE).

Менеджер нод stateless, всё хранится в базе данных.

API сервера выглядит при этом вот так:

```cpp
// создать новую ноду, записать ее в базу данных,
// присвоить ей id, создать JWE-ключ, вернуть их
POST /node/new {endpoint}

// запустить ноду
POST /node/start {node_id}

// остановить ноду
POST /node/stop {node_id}

// получить список нод c их статусами
GET /nodes [{node_id, endpoint, status}]

// создать пользователя, записать его в базу
// данных, присвоить ему id, вернуть его
POST /user/new {user_name}

// добавить пользователя на все ноды,
// добавить информацию об этом в базу данных
POST /user/enable {user_id}

// удалить пользователя их всех нод,
// добавить информацию об этом в базу данных
POST /user/disable {user_id}

// получить список пользователей
GET /users [{user_id, user_name, status}]

// получить список нод, на которых добавлен данный пользователь
GET /user/{user_id}-{user_name}/nodes
```

## База данных

Таблицы, которые существуют в базе данных:

**nodes** - ноды
- NodeID
- Endpoint
- Key
- ActualStatus
- RequiredStatus
- UserConfigTemplate

**users** - пользователи
- UserID
- Name
- Status
- CreationTime

**pending_operations** операции добавления/удаления пользователей на ноды, которые не были обработаны
- NodeID
- UserID
- Operation (add/delete)

## Сценарии:

### Добавление новой ноды
1. вызвать у node manager-а `/node/new {node_endpoint}`.
manager сгенерирует ключ, добавит его и endpoint в базу данных.
в response вернется `node_id` и ключ `node_key`.

2. скопировать ключ на ноду, запустить на ноде сервис, передав в параметрах этот ключ

### Старт ноды

1. вызвать у node manager-а `/node/start {node_id}`.

2. менеджер обращается к базе данных, получает список пользователей `users`, `node_endpoint` и `node_key`

3. менеджер отмечает в базе данных состояние ноды как неактуальное

4. менеджер делает запрос к ноде `node_endpoint/start {users}`, используя `node_key`. 

4. делаем запрос к базе данных, отмечаем там ноду как запущенную (если запрос был успешен), удаляем из `pending_operations` операции, относящиеся к ноде. отмечаем состояние ноды как актуальное.

худшее, что может быть - запрос к базе данных из 4 пункта не будет успешен, тогда состояние ноды в базе навечно останется там как неактуальное.

### Остановка ноды

1. вызывать у node manager-а `/node/stop {node_id}`.

2. менеджер обращается к базе данных, получает `node_endpoint` и `node_key`

3. менеджер отмечает в базе данных состояние ноды как неактуальное

4. менеджер делает запрос к ноде `node_endpoint/stop`, используя `node_key`. 

5. делаем запрос к базе данных, отмечаем там ноду как остановленную (если запрос был успешен), удаляем из `pending_operations` операции, относящиеся к ноде. отмечаем состояние ноды как актуальное.

худшее, что может быть - запрос к базе данных из 4 пункта не будет успешен, тогда состояние ноды в базе навечно останется там как неактуальное.

### Создание пользователя

1. вызвать у node manager-а `/user/new {user_name}

2. пользователь добавляется в базу, ему присваивается `user_id`

3. возвращаем user_id

### Включение пользователя

1. вызвать у node manager-а `/user/enable {user_id}`.

2. менеджер меняет в базе данных состояние пользователя на enabled,
добавляет в таблицу `pending operations` операции (add, user_id, node_id) для всех включенных нод

3. менеджер вызывает у каждой запущенной ноды метод `/users/edit` в который передает пользователей из `pending operations` для данной ноды

4. если вызов успешен, удаляем эти записи из `pending operations`. если удаление не удалось, ну, жаль, в бд остались неактуальные данные. возможно, их удастся актуализировать в будущем, когда мы заново обработаем penging operations для данной ноды

### Выключение пользователя

1. вызвать у node manager-а `/user/disable {user_id}`.

2. менеджер меняет в базе данных состояние пользователя на disabled,
добавляет в таблицу `pending operations` операции (del, user_id, node_id) для всех включенных нод

3. менеджер вызывает у каждой запущенной ноды метод `/users/edit` в который передает пользователей из `pending operations` для данной ноды

4. если вызов успешен, удаляем эти записи из `pending operations`. если удаление не удалось, ну, жаль, в бд остались неактуальные данные. возможно, их удастся актуализировать в будущем, когда мы заново обработаем penging operations для данной ноды

### Запрос актуальных нод для пользователя

1. вызвать у node manager-а `/user/{user_id}-{user_name}/nodes`

2. получить имя пользователя из базы данных, сравнить с переданным.
если передано что-то другое, вернуть ошибку доступа

3. получить из базы состояние пользователя и список pending operations, связанных с ним. сформировать, основываясь на этом, список доступных пользователю нод. 

### Heart-Beat для мониторинга нод

повторять каждые 3-5-10 секунд:

1. получить состояние ноды по api, вписать его в базу данных как ActualStatus.

2. если ActualStatus != RequiredStatus, попытаться его актуализировать (вызвать Start или Stop)

3. если ActualStatus == RequiredStatus, взять из базы pending operations, попытаться их применить

### Ожидаемая нагрузка

- 1-10 нод
- ~1000 пользователей
- 1 node-manager (в минимальной конфигурации - на том же сервере, где и единственная нода)
- 1-100 запросов в минуту на добавление/удаление пользователей
- запрос списка доступных нод - каждые 10 минут от каждого пользователя, итого ~10000 запросов в час, ~5 в секунду

### Что с отказами

Если нода становится недоступна, со временем node-manager об этом узнает (через Heart-Beat), удаляет её из списка доступных для клиентов, периодически пытается её перезапустить. Клиенты регулярно забирают список доступных нод, при очередном запросе её там не будет.

Если node manager перестанет быть доступным, то можно запустить точно такой же на другом сервере, поменять DNS-запись, ничего страшного.

Если база данных становится недоступна, то node-manager будет возвращать ошибки в ответ на клиентские запросы, клиенты будут пользоваться списком нод, который, по мере их падения, будет терять актуальность (перезапускать их master не будет). Если база данных в какой-то момент снова станет доступной - всё станет ок. Если не станет никогда и бэкапов не осталось - ну, всё, конец.

Из базы данных крайне важно сохранить список нод и список пользователей, пользуясь ими, сервис можно восстановить. 

### Что с базой данных

По мере развития сервиса будет, конечно, усложняться, но пока так:

Минимально (база данных, нода, nodemanager работает на одной vps): думаю, PostgreSQL с периодическими бэкапами в облако.

Если выяснится, что Postgres требует слишком много ресурсов, пробовать разное: добавить redis или собственный кэш для результатов запросов (самые частые запросы - список доступных нод, он ок кэшируется и редко меняется).

Чтобы сделать базу данных надёжнее, можно использовать какой-нибудь существующий сервис, и хостить её там, с бэкапами, репликами и всем таким.

---
---
---

# Структуры данных, коды ошибок, etc...

## XRay-Node

### Общие положения

- Все вызовы - идемпотентные, например, `/start` после `/start` не приведёт к ошибке, просто потому, что это удобно для вызывающего кода

- Все вызовы - требуют, чтобы содержимое запроса было зашифровано JWE-ключом, если это не так - возвращается `401 Unauthorized`

- Все вызовы, которые требуют данные, принимают их в JSON, при несовпадении (пришел не JSON, пришел JSON, но с не теми полями) - `400 Bad Request`

- Все ошибки возвращаются в JSON вот в таком формате:
  ```
  Content-Type: application/json
  {
    "error": "error", // string, required
    "message": "error details", // string, required
  }
  ```
- Все неожиданные ошибки с неясной причиной (например, json-encoder не смог записать ответ, gzip не смог ответ сархивировать, и прочее) - вот такой, ничего не говорящий ответ, детали только в логе:
  ```
  500 Internal Server Error
  {
    "error": "internal server error",
  }
  ```

  Ниже описаны ответы-ошибки, соответствующие специальным, документированным ситуациям:

### `POST /start { users }`

Запустить сервис с указанным списком пользователей

- `systemd/launchctl xray` сервис ещё не инициализирован:
  ```
  503 Service Unavailable
  {
    "error": "xray service initializing",
    "message": "Service is initializing, please try again later"
  }
  ```
- `systemd/systemctl start xray` вернул ошибку
  ```
  500 Internal Server Error
  {
    "error": "xray service error",
  }
  ```
- успешно завершилось - вернём характеристики ноды:
  ```
  200 OK
  {
    "userConfigTemplate": "NodeName config for server user {{.UserName}}"
  }
  ```

### `POST /stop`

Остановить сервис

- `systemd/launchctl xray` сервис ещё не инициализирован:
  ```
  503 Service Unavailable
  {
    "error": "xray service initializing",
    "message": "Service is initializing, please try again later"
  }
  ```
- `systemd/systemctl stop xray` вернул ошибку
  ```
  500 Internal Server Error
  {
    "error": "xray service error",
  }
  ```
- успешно завершилось - `200 OK, empty body`


### `GET /status`

Запросить статус сервиса (Running/Stopped)

- `systemd/launchctl xray` сервис ещё не инициализирован:
  ```
  503 Service Unavailable
  {
    "error": "xray service initializing",
    "message": "Service is initializing, please try again later"
  }
  ```
- `systemd/systemctl status xray` вернул ошибку
  ```
  500 Internal Server Error
  {
    "error": "xray service status error",
  }
  ```
- успешно завершилось - вернём статус ноды:
  ```
  200 OK
  {
    "nodeStatus": "Running"/"Stopped"
  }
  ```

### `POST /users/edit {add_users, del_users}`

Изменить список пользователей

- gRPC-API xray-сервиса не отвечает
  ```
  500 Internal Server Error
  {
    "error": "xray service unavailable",
  }
  ```

- gRPC-API xray-сервиса не отвечает (возможно, сервис упал или вообще не был запущен)
  ```
  500 Internal Server Error
  {
    "error": "xray service unavailable",
  }
  ```
- gRPC-API xray-сервиса отвечает ошибкой
  ```
  500 Internal Server Error
  {
    "error": "xray service error",
  }
  ```
- успешно завершилось - `200 OK`

## XRay-Manager

### Общие положения

те же, что и у XRay-Node выше +

все ошибки, связанные с недоступностью базы данных:

  ```
  500 Internal Server Error
  {
    "error": "database unavailable"
  }
  ```

### `POST /node/new {endpoint}`

Создать новую ноду, записать ее в базу данных,
присвоить ей id, создать JWE-ключ, вернуть их

- `endpoint` не является валидным адресом:
  ```
  400 Bad Request
  {
    "error": "invalid endpoint"
  }
  ```
- отработали ок (нода с таким endpoint либо уже существует, либо успешно добавлена в базу данных, сгенерирован JWE-ключ и тп):
  ```
  200 OK
  {
    "nodeID": "123",
    "nodeKey": "JWE Node Key"
  }
  ```

### `POST /node/start {nodeID}`

запустить ноду

- Ноды с таким ID не существует
  ```
  400 Bad Request
  {
    "error": "node not exists"
  }
  ```
- запрос к ноде вернул ошибку - вернем такой же ответ (см. метод `xray node POST /start`)

- вызов завершился успешно - `200 OK`

### `POST /node/stop {nodeID}`

Остановить ноду

- Ноды с таким ID не существует
  ```
  400 Bad Request
  {
    "error": "node not exists"
  }
  ```
- запрос к ноде вернул ошибку - вернем такой же ответ (см. метод `xray node POST /stop`)

- вызов завершился успешно - `200 OK`

### `GET /nodes`
получить список нод c их статусами

- вызов завершился успешно - вернем список нод с их текущими статусами 
  ```
  200 OK
  {
    "nodes" : [
        { 
            "id": "nodeID",
            "endpoint": "endpoint"
            "actualStatus": "Running"/"Stopped"/"Error"
            "requiredStatus": "Running"/"Stopped"
        }
    ],
  }
  ```

### `POST /user/new {user_name}`
создать пользователя, записать его в базу
данных, присвоить ему id, вернуть его
(имена не уникальны, вот этот запрос не идемпотентен,
будет создан новый пользователь с уникальным ID)

  ```
  200 OK
  {
    "user": { 
        "id": "userID",
        "name": "userName"
    }
  }
  ```

### `POST /user/enable {user_id}`
добавить пользователя на все ноды,
добавить информацию об этом в базу данных

- Пользователя с таким ID не существует
  ```
  400 Bad Request
  {
    "error": "user not exists"
  }
  ```
- Всё ок, пользователь добавился в базу, на какие-то ноды он уже добавлен, на какие-то будет добавляться фоново (они перезапустятся, если упали, база данных будет отражать это состояние) `200 OK`

### `POST /user/disable {user_id}`
удалить пользователя их всех нод,
добавить информацию об этом в базу данных

- Пользователя с таким ID не существует
  ```
  400 Bad Request
  {
    "error": "user not exists"
  }
  ```
- Всё ок, требуемое состояние пользователя записано в базу, с каких-то нод он уже добавлен, с какие-то будет удаляться фоново (они перезапустятся, если упали, база данных будет отражать это состояние) `200 OK`

### `GET /users`

- База данных доступна, всё ок
  ```
  200 OK
  {
    "users": [
      { 
        "id": "userID",
        "name": "userName"
        "status": "Enabled"/"Disabled"
      }
    ],
  }
  ```

### ```GET /user/{userID}-{userName}/nodes```

получить список нод, на которых добавлен данный пользователь

- `userID` не соответствует `userName`: вернём `422 Unprocessable Content` 

- всё ок, вспоминаем, что упоминали `UserConfigTempalte`, который каждая нода возвращает в ответ на /start, выбираем из базы данных список нод, на которые добавлен пользователь, берем их `UserConfigTempalte`, заполняем их с помощью `userID`, возвращаем список заполненных template-ов (в таком виде их принимают клиенты)

  ```
  200 OK
  [
    "NodeA specific config for server user userID",
    "NodeB specific config for server user userID",
    "NodeF specific config for server user userID"
  ]
  ```